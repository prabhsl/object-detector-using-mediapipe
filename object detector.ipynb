{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining model path for the model we are using, in this case we will use efficientdet_lite0 for object detection\n",
    "model_path = \"efficientdet_lite0.tflite\"\n",
    "\n",
    "# defining options for object detector\n",
    "base_options = python.BaseOptions(model_path)\n",
    "\n",
    "# score threshold allows us to set a threshold for confidence. if the detection made has a confidence of lower than 0.5 (50%) it will not be shown in the output.\n",
    "options = vision.ObjectDetectorOptions(base_options = base_options, score_threshold = 0.5)\n",
    "\n",
    "# initializing object detector\n",
    "detector = vision.ObjectDetector.create_from_options(options)\n",
    "\n",
    "image = mp.Image.create_from_file(\"cat and dog.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetectionResult(detections=[Detection(bounding_box=BoundingBox(origin_x=148, origin_y=403, width=363, height=603), categories=[Category(index=None, score=0.84375, display_name=None, category_name='cat')], keypoints=[]), Detection(bounding_box=BoundingBox(origin_x=463, origin_y=165, width=423, height=851), categories=[Category(index=None, score=0.81640625, display_name=None, category_name='dog')], keypoints=[])])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wreak\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:78: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "# this prints the results that are returned by the object detector. these results give us the coordinates of the bounding boxes and the name of the categories that are returned\n",
    "# we will use the output from our object detector in the visualization function\n",
    "detection_result = detector.detect((image))\n",
    "print(detection_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN = 10  # pixels\n",
    "ROW_SIZE = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "TEXT_COLOR = (255, 0, 0)  # red\n",
    "\n",
    "# visualization function\n",
    "def visualize(image, detection_result) -> np.ndarray: #this makes it so that the function returns a numpy ndarray\n",
    "  for detection in detection_result.detections:\n",
    "    # drawning bounding_box\n",
    "    bbox = detection.bounding_box\n",
    "    # defining starting point\n",
    "    start_point = bbox.origin_x, bbox.origin_y\n",
    "    # defining end points\n",
    "    end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "    cv2.rectangle(image, start_point, end_point, TEXT_COLOR, 3)\n",
    "\n",
    "    # drawing label(s) and score\n",
    "    category = detection.categories[0]\n",
    "    # getting names of the categories that are detected\n",
    "    category_name = category.category_name\n",
    "    # rounding off the score to the second decimal place\n",
    "    score = round(category.score, 2)\n",
    "    # merging category names and the score\n",
    "    result_text = category_name + ' (' + str(score) + ')'\n",
    "    # defining text location on the image\n",
    "    text_location = (MARGIN + bbox.origin_x, MARGIN + ROW_SIZE + bbox.origin_y)\n",
    "    # putting the text (category name and score) on the image\n",
    "    cv2.putText(image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN, FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.copy is being used to create a copy of the original image by using numpy.view() to get a new view of the image\n",
    "image_copy = np.copy(image.numpy_view())\n",
    "\n",
    "# calling the visualization function and passing copy of the original image and the object detector results to it\n",
    "annotated_image = visualize(image_copy, detection_result)\n",
    "# converting the annotated image from BGR to RGB. BGR is the default color space used by opencv\n",
    "annotated_image= cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# writes the image to the set path with the provided file name\n",
    "cv2.imwrite(\"obj detector output.png\",annotated_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
